feat: Enhance attention specialization analysis and add MLP specialization probes

This commit significantly improves the attention specialization breakdown notebook
and adds a new MLP activation specialization analysis.

## Attention Analysis Improvements

- **Model Training**: Added minimal training (2 epochs) for probe models to establish
  non-random structure while maintaining "probe, not optimizer" philosophy
- **Row Normalization**: Implemented per-feature row normalization (each row sums to 1)
  to make head preference visible, following standard mechanistic interpretability practices
- **Collapse Delta Panel**: Added visual collapse delta panel showing (Base 32 - Base 8)
  to directly demonstrate encoding-induced collapse without rhetoric
- **Enhanced Visualization**: Improved figure with color bars, better typography, feature
  names as y-axis labels, and summary statistics

## MLP Specialization Analysis (New)

- **MLP Activation Extraction**: Implemented forward hooks on `encoder_layer.linear1` to
  capture MLP activations (output before activation function)
- **Feature-Conditioned Variance Metric**: Created `compute_mlp_specialization()` function
  that measures variance ratio between feature-active and feature-inactive tokens
- **Cross-Base Comparison**: Added analysis loop to compute MLP specialization across all
  encoding bases (32, 24, 16, 8)
- **MLP Visualization**: Created heatmap visualization with dimension binning, row
  normalization, and collapse delta panel, using YlOrRd colormap for specialization
  patterns

## Technical Details

- MLP analysis uses d_ff dimensions (typically 4*d_model) from linear1 output
- Dimensions are binned into 32 bins for visualization
- Specialization score = active_variance / (inactive_variance + epsilon)
- All visualizations follow research-grade standards for mechanistic interpretability

## Files Modified

- `src/notebooks/04_attention_specialization_breakdown.ipynb`: Enhanced attention analysis
  and added complete MLP specialization analysis section

This work enables direct comparison between attention and MLP specialization patterns
under encoding-induced aliasing, providing comprehensive evidence for the collapse claim.
