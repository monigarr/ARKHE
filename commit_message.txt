fix: Correct metrics statistics after truncation in MetricsExporter

This commit fixes a bug in `record_inference_latency()` where statistics
(count, sum, min, max) were not recalculated after truncating the values
list to keep only the last 1000 entries. This caused increasingly inaccurate
average latency calculations as the number of recorded values exceeded 1000.

## Bug Fix

- **Metrics Truncation**: When `values` list exceeds 1000 items, statistics
  are now recalculated from the retained values only
- **Accurate Statistics**: `count`, `sum`, `min`, and `max` are now correctly
  updated after truncation, ensuring accurate average calculations
- **Edge Case Handling**: Properly handles empty values list after truncation

## Technical Details

- Before: Truncation removed old values but kept stale statistics
- After: Statistics are recalculated from retained values after truncation
- Impact: Fixes `sum / count` average calculation accuracy for long-running
  applications that record more than 1000 inference latencies

## Files Modified

- `src/math_research/utils/metrics.py`: Fixed `record_inference_latency()`
  to recalculate statistics after truncation

This fix ensures that Prometheus-compatible metrics remain accurate even
after extended operation periods with high inference volumes.
